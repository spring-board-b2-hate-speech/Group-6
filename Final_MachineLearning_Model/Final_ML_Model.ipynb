{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "HATE SPEECH DETECTION IN SNAPCHAT::\n",
        "\n",
        "we are going to develope a hate speech detection model using machine learning.\n"
      ],
      "metadata": {
        "id": "Fl7x1t_JSPDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have selected a dataset the info of dataset is available in Readme section of github repo."
      ],
      "metadata": {
        "id": "QZDkH34jWlMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- Now we are going to see all steps"
      ],
      "metadata": {
        "id": "1fu7QZhlW9W4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Cleaning and Analysis:\n",
        "\n",
        "Data cleaning involves preparing and cleaning the raw data to make it suitable for analysis. This includes handling missing values, removing special characters, converting text to lowercase, and more.\n",
        "\n",
        "--Data analysis helps us to  understand the characteristics of our data, identify patterns, and detect any issues that need to be addressed."
      ],
      "metadata": {
        "id": "wJeQQ2yEXL_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/HateSpeechDetection.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Display basic statistics about the dataset\n",
        "print(\"\\nBasic statistics of the dataset:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in the dataset:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check the distribution of the classes\n",
        "print(\"\\nDistribution of the classes:\")\n",
        "print(df['Comment'].value_counts())\n",
        "\n",
        "# Display the first few rows of the text data\n",
        "print(\"\\nFirst few rows of the text data:\")\n",
        "print(df['Comment'].head())\n",
        "\n",
        "\n",
        "# Removing special characters and converting text to lowercase\n",
        "df['Comment'] = df['Comment'].str.replace('[^a-zA-Z\\s]', '', regex=True).str.lower()\n",
        "\n",
        "# Display the cleaned data\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XbO6po9XSEA",
        "outputId": "f85134ec-8b50-49b2-aa09-5349a3fac3ed"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Platform                                            Comment  Hateful\n",
            "0   Reddit  Damn I thought they had strict gun laws in Ger...        0\n",
            "1   Reddit  I dont care about what it stands for or anythi...        0\n",
            "2   Reddit                  It's not a group it's an idea lol        0\n",
            "3   Reddit                          So it's not just America!        0\n",
            "4   Reddit  The dog is a spectacular dancer considering he...        0\n",
            "\n",
            "Basic statistics of the dataset:\n",
            "           Hateful\n",
            "count  3000.000000\n",
            "mean      0.200000\n",
            "std       0.400067\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       0.000000\n",
            "max       1.000000\n",
            "\n",
            "Missing values in the dataset:\n",
            "Platform    0\n",
            "Comment     0\n",
            "Hateful     0\n",
            "dtype: int64\n",
            "\n",
            "Distribution of the classes:\n",
            "Comment\n",
            "You can't make it on this list unless you believe in the future of #crypto, but we suspect next year's list will be even longer.                                     2\n",
            "Oh now he ain't Muslim or Arab ? You don't even want to claim your own people. But I get ain't nobody want to claim no dirty ass towelhead terrorist anyway          2\n",
            "Damn I thought they had strict gun laws in Germany                                                                                                                   1\n",
            "Humans have a natural aversion to using lethal force. It's why soldiers have to go through tons of training.                                                         1\n",
            "my phone is old, but with this wallpaper.. IT LOOKS NEW. i can't promote things well.                                                                                1\n",
            "                                                                                                                                                                    ..\n",
            "You are a strong woman u did a great job along your journey till now, I hope I can help u too                                                                        1\n",
            "Yui did a great job singing!                                                                                                                                         1\n",
            "no wonder you sucked on blond faggot                                                                                                                                 1\n",
            "Honestly, I'm a little upset that on my horseless playthrough, Leyndall won't let me in, despite having the Great Runes! What's a Rune Level 1 brother gotta do?!    1\n",
            "Desecrate men by making them gay/trannies, and women by making them abort children and become psycho feminist cunts etc                                              1\n",
            "Name: count, Length: 2998, dtype: int64\n",
            "\n",
            "First few rows of the text data:\n",
            "0    Damn I thought they had strict gun laws in Ger...\n",
            "1    I dont care about what it stands for or anythi...\n",
            "2                    It's not a group it's an idea lol\n",
            "3                            So it's not just America!\n",
            "4    The dog is a spectacular dancer considering he...\n",
            "Name: Comment, dtype: object\n",
            "  Platform                                            Comment  Hateful\n",
            "0   Reddit  damn i thought they had strict gun laws in ger...        0\n",
            "1   Reddit  i dont care about what it stands for or anythi...        0\n",
            "2   Reddit                    its not a group its an idea lol        0\n",
            "3   Reddit                            so its not just america        0\n",
            "4   Reddit  the dog is a spectacular dancer considering he...        0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Data Preprocessing:\n",
        "\n",
        "--Tokenizatin and count Vectorization:\n",
        "--Label Encoding:\n",
        "\n",
        "Data preprocessing includes steps like tokenization (splitting text into words), label encoding (converting categorical labels into numeric values), and count vectorization (converting text into a matrix of token counts).\n",
        "\n"
      ],
      "metadata": {
        "id": "hcqfZ2_nXjQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "\n",
        "# Inspect Dataset and Check for Missing Values\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"\\nBasic information about the dataset:\")\n",
        "print(df.info())\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values in each column:\\n\", missing_values)\n",
        "\n",
        "# Display the first few rows of the text column\n",
        "# Assuming the text column is named 'processed_text' and the label column is named 'label'\n",
        "text_column = 'Comment'\n",
        "label_column = 'label'\n",
        "\n",
        "print(\"\\nFirst few rows of the text column:\")\n",
        "print(df[text_column].head())\n",
        "\n",
        "# Tokenize the text data\n",
        "df['tokenized_text'] = df[text_column].apply(word_tokenize)\n",
        "\n",
        "# Display the first few rows of the tokenized text column\n",
        "print(\"\\nFirst few rows of the tokenized text column:\")\n",
        "print(df['tokenized_text'].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVjMcO13X9XP",
        "outputId": "d6c66fa0-d906-4f9b-cdfe-706e14cfbc96"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Basic information about the dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Platform  3000 non-null   object\n",
            " 1   Comment   3000 non-null   object\n",
            " 2   Hateful   3000 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 70.4+ KB\n",
            "None\n",
            "\n",
            "Missing values in each column:\n",
            " Platform    0\n",
            "Comment     0\n",
            "Hateful     0\n",
            "dtype: int64\n",
            "\n",
            "First few rows of the text column:\n",
            "0    damn i thought they had strict gun laws in ger...\n",
            "1    i dont care about what it stands for or anythi...\n",
            "2                      its not a group its an idea lol\n",
            "3                              so its not just america\n",
            "4    the dog is a spectacular dancer considering he...\n",
            "Name: Comment, dtype: object\n",
            "\n",
            "First few rows of the tokenized text column:\n",
            "0    [damn, i, thought, they, had, strict, gun, law...\n",
            "1    [i, dont, care, about, what, it, stands, for, ...\n",
            "2             [its, not, a, group, its, an, idea, lol]\n",
            "3                        [so, its, not, just, america]\n",
            "4    [the, dog, is, a, spectacular, dancer, conside...\n",
            "5    [if, ppl, dont, wear, masks, you, complain, if...\n",
            "6      [we, should, send, them, all, back, to, africa]\n",
            "7    [checking, to, see, if, its, whataboutism, fro...\n",
            "8                  [as, a, european, i, approve, this]\n",
            "9    [idk, which, of, these, groups, to, join, they...\n",
            "Name: tokenized_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--LABEL Encoding:"
      ],
      "metadata": {
        "id": "g-Y9G6fXZ5q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode the 'Platform' Column using Label Encoding\n",
        "\n",
        "Label Encoding transforms categorical values into integers. This is particularly useful for machine learning algorithms that cannot work with categorical data directly.\n",
        "\n",
        "\n",
        "\n",
        "--Encode the 'Comment' Column using Count Vectorization.\n",
        "\n",
        "Count Vectorization converts the text into a matrix of token counts. Each word in the text is represented as a feature.\n",
        "\n",
        "--Combine Encoded Features.\n",
        "\n",
        "We merge the encoded Platform column and the count vectorized Comment data into a single DataFrame."
      ],
      "metadata": {
        "id": "CjWaglVKaL2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = pd.read_csv('/content/HateSpeechDetection.csv')\n",
        "\n",
        "# Label Encode the 'Platform' column\n",
        "label_encoder = LabelEncoder()\n",
        "data['Platform_encoded'] = label_encoder.fit_transform(data['Platform'])\n",
        "\n",
        "# Display the encoded platform data\n",
        "print(data[['Platform', 'Platform_encoded']].head())\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Count Vectorize the 'Comment' column\n",
        "count_vectorizer = CountVectorizer(max_features=5000)\n",
        "comments_count = count_vectorizer.fit_transform(data['Comment']).toarray()\n",
        "\n",
        "# Convert to DataFrame\n",
        "comments_count_df = pd.DataFrame(comments_count, columns=count_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display the encoded comments data\n",
        "print(comments_count_df.head())\n",
        "\n",
        "# Combine all features into a single DataFrame\n",
        "encoded_data = pd.concat([comments_count_df, data['Platform_encoded']], axis=1)\n",
        "encoded_data['Hateful'] = data['Hateful']\n",
        "\n",
        "# Display the combined encoded data\n",
        "print(encoded_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPmA9NNxaNgJ",
        "outputId": "abf751f9-21e5-49e4-e000-594367383eaf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Platform  Platform_encoded\n",
            "0   Reddit                 1\n",
            "1   Reddit                 1\n",
            "2   Reddit                 1\n",
            "3   Reddit                 1\n",
            "4   Reddit                 1\n",
            "   000  00am  02  10  100  1000001  100k  105  10days  10k  ...  zipped  \\\n",
            "0    0     0   0   0    0        0     0    0       0    0  ...       0   \n",
            "1    0     0   0   0    0        0     0    0       0    0  ...       0   \n",
            "2    0     0   0   0    0        0     0    0       0    0  ...       0   \n",
            "3    0     0   0   0    0        0     0    0       0    0  ...       0   \n",
            "4    0     0   0   0    0        0     0    0       0    0  ...       0   \n",
            "\n",
            "   zipper  zoe  zombie  zone  zonked  zoology  zoomies  zuckerberg  äôt  \n",
            "0       0    0       0     0       0        0        0           0    0  \n",
            "1       0    0       0     0       0        0        0           0    0  \n",
            "2       0    0       0     0       0        0        0           0    0  \n",
            "3       0    0       0     0       0        0        0           0    0  \n",
            "4       0    0       0     0       0        0        0           0    0  \n",
            "\n",
            "[5 rows x 5000 columns]\n",
            "   000  00am  02  10  100  1000001  100k  105  10days  10k  ...  zoe  zombie  \\\n",
            "0    0     0   0   0    0        0     0    0       0    0  ...    0       0   \n",
            "1    0     0   0   0    0        0     0    0       0    0  ...    0       0   \n",
            "2    0     0   0   0    0        0     0    0       0    0  ...    0       0   \n",
            "3    0     0   0   0    0        0     0    0       0    0  ...    0       0   \n",
            "4    0     0   0   0    0        0     0    0       0    0  ...    0       0   \n",
            "\n",
            "   zone  zonked  zoology  zoomies  zuckerberg  äôt  Platform_encoded  Hateful  \n",
            "0     0       0        0        0           0    0                 1        0  \n",
            "1     0       0        0        0           0    0                 1        0  \n",
            "2     0       0        0        0           0    0                 1        0  \n",
            "3     0       0        0        0           0    0                 1        0  \n",
            "4     0       0        0        0           0    0                 1        0  \n",
            "\n",
            "[5 rows x 5002 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.-- Handling DATA Imbalance with SMOTE\n",
        "\n",
        "\n",
        "SMOTE (Synthetic Minority Over-sampling Technique) is used to address class imbalance by generating synthetic samples for the minority class."
      ],
      "metadata": {
        "id": "9aF2zn__bW3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Separate features and target\n",
        "X = data['Comment']\n",
        "y = data['Hateful']\n",
        "\n",
        "# Convert text data to numeric using TF-IDF or CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_smote, y_smote = smote.fit_resample(X_vectorized, y)\n",
        "\n",
        "# Display new class counts\n",
        "print(pd.Series(y_smote).value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crx-h3PObZjD",
        "outputId": "ef734269-6be7-471f-925c-91958e2a07c4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hateful\n",
            "0    2400\n",
            "1    2400\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4,---Splitting the Data:\n",
        "Splitting the dataset involves dividing the data into training and testing sets to evaluate the model's performance."
      ],
      "metadata": {
        "id": "ybi2dHOhcckL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features and labels\n",
        "X = df['Comment']\n",
        "y = df['Hateful']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'Training samples: {len(X_train)}')\n",
        "print(f'Testing samples: {len(X_test)}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6XMHLg6cn4L",
        "outputId": "4237b9a2-7a0f-44d8-9251-aefca21659c2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 2400\n",
            "Testing samples: 600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5,-----Model Selection with Logistic Regression:\n",
        "\n",
        "\n",
        " Logistic regression is chosen for its simplicity and effectiveness for binary classification tasks."
      ],
      "metadata": {
        "id": "A-Ws3XJ-cByh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "# Define the model and hyperparameters\n",
        "lr_model = LogisticRegression()\n",
        "lr_params = {'C': [0.01, 0.1, 1, 10, 100,1000]}\n",
        "\n",
        "# Perform grid search\n",
        "lr_grid = GridSearchCV(lr_model, lr_params, cv=5, scoring='accuracy')\n",
        "lr_grid.fit(X_train_vec, y_train)\n",
        "\n",
        "# Make predictions\n",
        "lr_predictions = lr_grid.predict(X_test_vec)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X76MH3fxcJa2",
        "outputId": "193033f3-fcf5-4034-87f2-cc0feca47f6d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--6,Evaluating the Model:"
      ],
      "metadata": {
        "id": "hpL334rSd5LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
        "print('Logistic Regression Accuracy:', lr_accuracy)\n",
        "print('Best Parameters:', lr_grid.best_params_)\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "print('Model Performance Summary:')\n",
        "print(f'Logistic Regression Accuracy: {lr_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYQ_6Gg2eGwv",
        "outputId": "ce809378-2b0a-4a58-e5ce-313a1a07c9f6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.9433333333333334\n",
            "Best Parameters: {'C': 1000}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.99      0.97       494\n",
            "           1       0.93      0.74      0.82       106\n",
            "\n",
            "    accuracy                           0.94       600\n",
            "   macro avg       0.94      0.86      0.89       600\n",
            "weighted avg       0.94      0.94      0.94       600\n",
            "\n",
            "Model Performance Summary:\n",
            "Logistic Regression Accuracy: 0.9433333333333334\n"
          ]
        }
      ]
    }
  ]
}