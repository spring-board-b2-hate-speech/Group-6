{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Importing Libraries and Loading the Data:\n",
        ""
      ],
      "metadata": {
        "id": "NQEucZyHbLNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Conv1D, MaxPooling1D, Dense, Dropout, SpatialDropout1D, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/HateSpeechDetection (Balanced dataset).csv'  # Update the file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "typ3NFUKdisF",
        "outputId": "20fe4a10-4efe-4d62-a59d-db3e94ea07cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Platform                                            Comment  Hateful\n",
            "0   Reddit  Damn I thought they had strict gun laws in Ger...        0\n",
            "1   Reddit  I dont care about what it stands for or anythi...        0\n",
            "2   Reddit                  It's not a group it's an idea lol        0\n",
            "3   Reddit                          So it's not just America!        0\n",
            "4   Reddit  The dog is a spectacular dancer considering he...        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing:"
      ],
      "metadata": {
        "id": "9czkfq-bds1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Apply the preprocessing function to the comments\n",
        "df['Comment'] = df['Comment'].apply(preprocess_text)\n",
        "\n",
        "# Display the first few rows of the preprocessed dataframe\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVMoOfj6duOh",
        "outputId": "6b24a338-a05d-4bbd-98ca-7abfcbc96f3b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Platform                                           Comment  Hateful\n",
            "0   Reddit              damn thought strict gun laws germany        0\n",
            "1   Reddit  dont care stands anything connected like shields        0\n",
            "2   Reddit                                    group idea lol        0\n",
            "3   Reddit                                           america        0\n",
            "4   Reddit  dog spectacular dancer considering two left feet        0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization and Padding:\n"
      ],
      "metadata": {
        "id": "jqBPEGg_d4J6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features and labels\n",
        "X = df['Comment']\n",
        "y = df['Hateful']\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_tokenized = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Pad the sequences\n",
        "max_length = 100  # Define the maximum length for padding\n",
        "X_padded = pad_sequences(X_tokenized, maxlen=max_length, padding='post')\n",
        "\n",
        "# Display the shape of the padded data\n",
        "print(f'Padded data shape: {X_padded.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45xgL1NJd6nS",
        "outputId": "c9b6d96b-825d-41bd-cba5-4e88730302b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded data shape: (3000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balancing the Dataset with RandomOverSampler:"
      ],
      "metadata": {
        "id": "IecupatNeD8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use RandomOverSampler to balance the dataset\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_padded, y)\n",
        "\n",
        "# Display the shape of the resampled data\n",
        "print(f'Resampled data shape: {X_resampled.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BY6-u3deHEM",
        "outputId": "5535d2c4-ada1-4515-fde0-d80cffa1aef3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resampled data shape: (4800, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the Data:"
      ],
      "metadata": {
        "id": "NOPVp9pzeLB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the resampled data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'Training samples: {len(X_train)}')\n",
        "print(f'Testing samples: {len(X_test)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ20jCyCeNM1",
        "outputId": "9cf33755-ce6c-467d-b81f-cebc62b046ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 3840\n",
            "Testing samples: 960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Model:"
      ],
      "metadata": {
        "id": "9ToVK16HeSUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "embedding_dim = 128  # Dimension of the embedding vectors\n",
        "model = Sequential()\n",
        "\n",
        "# Add embedding layer\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_length))\n",
        "\n",
        "# Add spatial dropout layer\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "\n",
        "# Add convolutional layers\n",
        "model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Add bidirectional LSTM layer\n",
        "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
        "\n",
        "# Add dense layers\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWcsy2C6eVU3",
        "outputId": "125f2ad2-a1d1-4b58-dd1f-1b31443769b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 128)          792704    \n",
            "                                                                 \n",
            " spatial_dropout1d (Spatial  (None, 100, 128)          0         \n",
            " Dropout1D)                                                      \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 96, 64)            41024     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 48, 64)            0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 200)               132000    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               20100     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 985929 (3.76 MB)\n",
            "Trainable params: 985929 (3.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model:"
      ],
      "metadata": {
        "id": "DNBO81JxebOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlFEK9fEecg9",
        "outputId": "dda29968-5b05-4daf-fb2c-9c09a4106f45"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "48/48 [==============================] - 32s 511ms/step - loss: 0.6684 - accuracy: 0.5775 - val_loss: 0.4600 - val_accuracy: 0.8333\n",
            "Epoch 2/10\n",
            "48/48 [==============================] - 17s 346ms/step - loss: 0.1662 - accuracy: 0.9388 - val_loss: 0.0823 - val_accuracy: 0.9740\n",
            "Epoch 3/10\n",
            "48/48 [==============================] - 18s 386ms/step - loss: 0.0165 - accuracy: 0.9958 - val_loss: 0.1084 - val_accuracy: 0.9727\n",
            "Epoch 4/10\n",
            "48/48 [==============================] - 17s 360ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 0.1045 - val_accuracy: 0.9727\n",
            "Epoch 5/10\n",
            "48/48 [==============================] - 17s 345ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.1282 - val_accuracy: 0.9701\n",
            "Epoch 6/10\n",
            "48/48 [==============================] - 16s 333ms/step - loss: 6.1649e-04 - accuracy: 1.0000 - val_loss: 0.1110 - val_accuracy: 0.9779\n",
            "Epoch 7/10\n",
            "48/48 [==============================] - 16s 333ms/step - loss: 3.5146e-04 - accuracy: 1.0000 - val_loss: 0.1390 - val_accuracy: 0.9714\n",
            "Epoch 8/10\n",
            "48/48 [==============================] - 18s 373ms/step - loss: 1.5133e-04 - accuracy: 1.0000 - val_loss: 0.1447 - val_accuracy: 0.9714\n",
            "Epoch 9/10\n",
            "48/48 [==============================] - 20s 422ms/step - loss: 8.3352e-04 - accuracy: 0.9997 - val_loss: 0.1640 - val_accuracy: 0.9661\n",
            "Epoch 10/10\n",
            "48/48 [==============================] - 16s 323ms/step - loss: 3.0635e-04 - accuracy: 1.0000 - val_loss: 0.1412 - val_accuracy: 0.9727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the Model:"
      ],
      "metadata": {
        "id": "CWVYFcWPegJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdAeZ65PeifG",
        "outputId": "454e6716-4f9c-4b4b-a1be-6aca3173bc3c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30/30 [==============================] - 4s 104ms/step\n",
            "30/30 [==============================] - 5s 89ms/step - loss: 0.1334 - accuracy: 0.9750\n",
            "Test Accuracy: 0.9750000238418579\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97       481\n",
            "           1       0.97      0.99      0.98       479\n",
            "\n",
            "    accuracy                           0.97       960\n",
            "   macro avg       0.98      0.98      0.97       960\n",
            "weighted avg       0.98      0.97      0.97       960\n",
            "\n"
          ]
        }
      ]
    }
  ]
}