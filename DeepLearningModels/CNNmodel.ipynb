{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CNN model:   "
      ],
      "metadata": {
        "id": "gexS9qh_hnYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "# For data manipulation and analysis\n",
        "import numpy as np\n",
        " # For numerical computations\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# For converting text into sequences of integers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        " # For padding sequences to the same length\n",
        "from tensorflow.keras.models import Sequential\n",
        "  # For creating a linear stack of neural network layers\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Dense, Dropout, SpatialDropout1D, Flatten\n",
        " # Different layers for the neural network\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# Optimizer for the model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        " # To stop training when a monitored metric stops improving\n",
        "from tensorflow.keras.regularizers import l2\n",
        " # Regularizer to prevent overfitting\n",
        "from sklearn.model_selection import train_test_split\n",
        " # For splitting data into training and testing sets\n",
        "from sklearn.metrics import classification_report\n",
        "# For generating a report showing the main classification metrics\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        " # For balancing the dataset by oversampling\n",
        "import nltk\n",
        "  # For natural language processing\n",
        "from nltk.corpus import stopwords\n",
        " # For removing common words that do not carry significant meaning\n",
        "import string\n",
        " # For string operations\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')  # Download the list of stopwords from NLTK\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/HateSpeechDetection (Balanced dataset).csv'  # File path to the dataset\n",
        "df = pd.read_csv(file_path)  # Load the dataset into a pandas DataFrame\n",
        "\n",
        "# Data Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    stop_words = set(stopwords.words('english'))  # Set of stopwords in English\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "# Apply the preprocessing function to the comments\n",
        "df['Comment'] = df['Comment'].apply(preprocess_text)  # Preprocess the 'Comment' column\n",
        "\n",
        "# Split the data into features and labels\n",
        "X = df['Comment']  # Features are the comments\n",
        "y = df['Hateful']  # Labels are whether the comment is hateful or not\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()  # Initialize the Tokenizer\n",
        "tokenizer.fit_on_texts(X)  # Fit the tokenizer on the text data\n",
        "X_tokenized = tokenizer.texts_to_sequences(X)  # Convert text to sequences of integers\n",
        "\n",
        "# Pad the sequences\n",
        "max_length = 100  # Define maximum length of sequences\n",
        "X_padded = pad_sequences(X_tokenized, maxlen=max_length, padding='post')  # Pad sequences to the same length\n",
        "\n",
        "# Use RandomOverSampler to balance the dataset\n",
        "desired_samples_per_class = 5000  # Desired number of samples per class\n",
        "sampling_strategy = {0: desired_samples_per_class, 1: desired_samples_per_class}  # Define sampling strategy\n",
        "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)  # Initialize RandomOverSampler\n",
        "X_resampled, y_resampled = ros.fit_resample(X_padded, y)  # Resample the dataset\n",
        "\n",
        "# Split the resampled data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)  # Split data\n",
        "\n",
        "# Define the CNN model\n",
        "embedding_dim = 32  # Define embedding dimension\n",
        "cnn_model = Sequential()  # Initialize the Sequential model\n",
        "cnn_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_length))  # Add embedding layer\n",
        "cnn_model.add(SpatialDropout1D(0.2))  # Add spatial dropout layer\n",
        "cnn_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))  # Add Conv1D layer\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))  # Add max pooling layer\n",
        "cnn_model.add(Flatten())  # Flatten the input\n",
        "cnn_model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))  # Add dense layer with L2 regularization\n",
        "cnn_model.add(Dropout(0.5))  # Add dropout layer\n",
        "cnn_model.add(Dense(1, activation='sigmoid'))  # Add output layer\n",
        "\n",
        "# Compile the CNN model\n",
        "optimizer = Adam(learning_rate=0.001)  # Define the optimizer\n",
        "cnn_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])  # Compile the model\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)  # Early stopping\n",
        "\n",
        "# Train the CNN model\n",
        "cnn_history = cnn_model.fit(X_train, y_train, epochs=3, batch_size=64, validation_split=0.2, verbose=1, callbacks=[early_stopping])  # Train the model\n",
        "\n",
        "# Evaluate the CNN model\n",
        "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test, y_test)  # Evaluate the model\n",
        "print(f'Test Accuracy: {cnn_accuracy}')  # Print test accuracy\n",
        "\n",
        "# Generate predictions and print classification report\n",
        "y_pred = (cnn_model.predict(X_test) > 0.5).astype(\"int32\")  # Generate predictions\n",
        "print(classification_report(y_test, y_pred))  # Print classification report\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwQQTNoCyhay",
        "outputId": "6fee9627-0059-40d2-eda9-f3b838078ad4"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (5000) in class 0 will be larger than the number of samples in the majority class (class #0 -> 2400)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (5000) in class 1 will be larger than the number of samples in the majority class (class #0 -> 2400)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 3s 18ms/step - loss: 0.8081 - accuracy: 0.5584 - val_loss: 0.6566 - val_accuracy: 0.8331\n",
            "Epoch 2/3\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.3449 - accuracy: 0.9097 - val_loss: 0.1210 - val_accuracy: 0.9900\n",
            "Epoch 3/3\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.1266 - accuracy: 0.9844 - val_loss: 0.0657 - val_accuracy: 0.9987\n",
            "63/63 [==============================] - 1s 6ms/step - loss: 0.0700 - accuracy: 0.9950\n",
            "Test Accuracy: 0.9950000047683716\n",
            "63/63 [==============================] - 0s 5ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99      1001\n",
            "           1       0.99      1.00      1.00       999\n",
            "\n",
            "    accuracy                           0.99      2000\n",
            "   macro avg       1.00      1.00      0.99      2000\n",
            "weighted avg       1.00      0.99      0.99      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again building the model by following these steps:\n",
        "\n",
        "\n",
        "\n",
        "Further reducing the number of filters in Conv1D.\n",
        "\n",
        "Further reducing the number of neurons in Dense layers.\n",
        "\n",
        "Reducing the learning rate.\n",
        "\n",
        "Increasing the dropout rate.\n",
        "\n",
        "Simplify the model architecture.\n"
      ],
      "metadata": {
        "id": "NDSdEVHRzarG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/HateSpeechDetection (Balanced dataset).csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "3m_nlKJCy7EA",
        "outputId": "6dffe161-5913-48c5-dfa2-669de47a0d81"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Platform                                            Comment  Hateful\n",
              "0   Reddit  Damn I thought they had strict gun laws in Ger...        0\n",
              "1   Reddit  I dont care about what it stands for or anythi...        0\n",
              "2   Reddit                  It's not a group it's an idea lol        0\n",
              "3   Reddit                          So it's not just America!        0\n",
              "4   Reddit  The dog is a spectacular dancer considering he...        0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21c153ec-ed5a-4668-8356-39b35c28da5e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Platform</th>\n",
              "      <th>Comment</th>\n",
              "      <th>Hateful</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Reddit</td>\n",
              "      <td>Damn I thought they had strict gun laws in Ger...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Reddit</td>\n",
              "      <td>I dont care about what it stands for or anythi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Reddit</td>\n",
              "      <td>It's not a group it's an idea lol</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Reddit</td>\n",
              "      <td>So it's not just America!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reddit</td>\n",
              "      <td>The dog is a spectacular dancer considering he...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21c153ec-ed5a-4668-8356-39b35c28da5e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-21c153ec-ed5a-4668-8356-39b35c28da5e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-21c153ec-ed5a-4668-8356-39b35c28da5e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5fed5868-f7fd-48be-8e2e-2dcbab95c242\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5fed5868-f7fd-48be-8e2e-2dcbab95c242')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5fed5868-f7fd-48be-8e2e-2dcbab95c242 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3000,\n  \"fields\": [\n    {\n      \"column\": \"Platform\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Reddit\",\n          \"Twitter\",\n          \"4Chan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2998,\n        \"samples\": [\n          \"If you could or heard his laugh when you replied that!\",\n          \"haha yes that man who trained so hard and worked so hard that he won the race just happened to let himself celebrate his achievement a little too soon, so suddenly he's all about ego and has zero determination.\",\n          \"If anyone looks like a monkey it's that ugly cunt!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Hateful\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Data Preprocessing"
      ],
      "metadata": {
        "id": "1Y22R5aeib8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "\n",
        "# Apply the preprocessing function to the comments\n",
        "df['Comment'] = df['Comment'].apply(preprocess_text)\n",
        "print(\"Data preprocessing completed.\")\n",
        "print(df.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNr_MFzcifiL",
        "outputId": "a0eb2c8c-4860-4c77-f163-69f106644390"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing completed.\n",
            "  Platform                                           Comment  Hateful\n",
            "0   Reddit              damn thought strict gun laws germany        0\n",
            "1   Reddit  dont care stands anything connected like shields        0\n",
            "2   Reddit                                    group idea lol        0\n",
            "3   Reddit                                           america        0\n",
            "4   Reddit  dog spectacular dancer considering two left feet        0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize and Pad Sequences"
      ],
      "metadata": {
        "id": "Dki3eotcjAME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features and labels\n",
        "X = df['Comment']\n",
        "y = df['Hateful']\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_tokenized = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Pad the sequences\n",
        "max_length = 100\n",
        "X_padded = pad_sequences(X_tokenized, maxlen=max_length, padding='post')\n",
        "print(\"Data preprocessing completed.\")\n",
        "print(df.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRqlGEL1jBZI",
        "outputId": "86378bc1-5d40-467e-9406-3edde9a52b2f"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing completed.\n",
            "  Platform                                           Comment  Hateful\n",
            "0   Reddit              damn thought strict gun laws germany        0\n",
            "1   Reddit  dont care stands anything connected like shields        0\n",
            "2   Reddit                                    group idea lol        0\n",
            "3   Reddit                                           america        0\n",
            "4   Reddit  dog spectacular dancer considering two left feet        0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Tokenize and Pad Sequences"
      ],
      "metadata": {
        "id": "2jZ_DwuBkHXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features and labels\n",
        "X = df['Comment']\n",
        "y = df['Hateful']\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_tokenized = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Pad the sequences\n",
        "max_length = 100\n",
        "X_padded = pad_sequences(X_tokenized, maxlen=max_length, padding='post')\n",
        "print(\"Tokenization and padding completed.\")\n",
        "print(X_padded[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdnfEwmujGrZ",
        "outputId": "58cd55b7-7ddc-4580-8fb2-263aa3e716ed"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization and padding completed.\n",
            "[[ 190  148 2423 1488 1489  581    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   2   57 1071   93 1490    1 2424    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [ 850  251   58    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [ 176    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [ 280 2425 2426  690  149  420  851    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balance the Dataset:"
      ],
      "metadata": {
        "id": "H-UBacZikT3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use RandomOverSampler to balance the dataset\n",
        "desired_samples_per_class = 5000\n",
        "sampling_strategy = {0: desired_samples_per_class, 1: desired_samples_per_class}\n",
        "\n",
        "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_padded, y)\n",
        "# Print the results\n",
        "print(\"Dataset balanced using RandomOverSampler.\")\n",
        "print(\"Class distribution after resampling:\")\n",
        "print(pd.Series(y_resampled).value_counts())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFoz8bfvkWQ_",
        "outputId": "2ddf91d3-1729-4100-cffc-aa1ffc8c7396"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset balanced using RandomOverSampler.\n",
            "Class distribution after resampling:\n",
            "Hateful\n",
            "0    5000\n",
            "1    5000\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (5000) in class 0 will be larger than the number of samples in the majority class (class #0 -> 2400)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (5000) in class 1 will be larger than the number of samples in the majority class (class #0 -> 2400)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Split the Data"
      ],
      "metadata": {
        "id": "MUrqTa96kesA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the resampled data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "print(\"Data split into training and testing sets.\")\n",
        "print(f\"Training set size: {len(X_train)}, Testing set size: {len(X_test)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjeQP-ANkgex",
        "outputId": "29ce2122-3211-4b88-de9c-7ce36542e536"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split into training and testing sets.\n",
            "Training set size: 8000, Testing set size: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Model:"
      ],
      "metadata": {
        "id": "L5ZpaNLskmco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "embedding_dim = 32  # Further reduced embedding dimension\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_length))\n",
        "cnn_model.add(Conv1D(filters=16, kernel_size=5, activation='relu'))  # Further reduced filters\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))  # Further reduced neurons\n",
        "cnn_model.add(Dropout(0.3))  # Increased dropout rate\n",
        "cnn_model.add(Dense(1, activation='sigmoid'))\n",
        "print(\"Model defined successfully.\")\n",
        "cnn_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_OBK5T7knuJ",
        "outputId": "52abcba7-2a1d-4c67-8827-c5fc1523891e"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model defined successfully.\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_27 (Embedding)    (None, 100, 32)           198176    \n",
            "                                                                 \n",
            " conv1d_38 (Conv1D)          (None, 96, 16)            2576      \n",
            "                                                                 \n",
            " max_pooling1d_23 (MaxPooli  (None, 48, 16)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 768)               0         \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 32)                24608     \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 225393 (880.44 KB)\n",
            "Trainable params: 225393 (880.44 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the Model"
      ],
      "metadata": {
        "id": "OQq4Jsryk4Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compile the CNN model\n",
        "optimizer = Adam(learning_rate=0.0001)  # Reduced learning rate\n",
        "cnn_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "D8vA-op7k5MQ"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Model"
      ],
      "metadata": {
        "id": "Ahza12uIk_CW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "\n",
        "# Train the CNN model\n",
        "cnn_history = cnn_model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2, verbose=1, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAqozziWsqg0",
        "outputId": "6ffc74db-2cee-4037-8e2b-734e24bb0932"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "100/100 [==============================] - 2s 14ms/step - loss: 1.2167 - accuracy: 0.5263 - val_loss: 1.1299 - val_accuracy: 0.5663\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 1.0613 - accuracy: 0.5861 - val_loss: 0.9984 - val_accuracy: 0.6662\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 2s 19ms/step - loss: 0.9474 - accuracy: 0.6722 - val_loss: 0.9007 - val_accuracy: 0.7475\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.8585 - accuracy: 0.7620 - val_loss: 0.8194 - val_accuracy: 0.8006\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.7684 - accuracy: 0.8448 - val_loss: 0.7153 - val_accuracy: 0.8625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the Model"
      ],
      "metadata": {
        "id": "cb-0PjI4lJrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the CNN model\n",
        "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {cnn_accuracy}')\n",
        "\n",
        "# Generate predictions and print classification report\n",
        "y_pred = (cnn_model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EanGCMTnr6Bk",
        "outputId": "b0165c6e-ec03-40d7-a4ca-5f71e22b009e"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 0s 3ms/step - loss: 0.7117 - accuracy: 0.8665\n",
            "Test Accuracy: 0.8665000200271606\n",
            "63/63 [==============================] - 0s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.96      0.88      1001\n",
            "           1       0.95      0.77      0.85       999\n",
            "\n",
            "    accuracy                           0.87      2000\n",
            "   macro avg       0.88      0.87      0.87      2000\n",
            "weighted avg       0.88      0.87      0.87      2000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}