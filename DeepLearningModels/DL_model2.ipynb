{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To address the issue of the model not correctly identifying hateful comments, we can try a few additional strategies beyond using class weights. These include oversampling the minority class and tweaking the model architecture. We'll also increase the number of epochs and batch size for better training."
      ],
      "metadata": {
        "id": "ReViMI_wMAWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Importing Libraries and Loading the Data:"
      ],
      "metadata": {
        "id": "r8rYL-MnMEfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Conv1D, MaxPooling1D, Dense, Dropout, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/HateSpeechDetection (Balanced dataset).csv'  # Update the file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_wkq3r4MKJ-",
        "outputId": "b8992294-9ecd-47ed-ecbe-052d4b72054f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Platform                                            Comment  Hateful\n",
            "0   Reddit  Damn I thought they had strict gun laws in Ger...        0\n",
            "1   Reddit  I dont care about what it stands for or anythi...        0\n",
            "2   Reddit                  It's not a group it's an idea lol        0\n",
            "3   Reddit                          So it's not just America!        0\n",
            "4   Reddit  The dog is a spectacular dancer considering he...        0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing:\n"
      ],
      "metadata": {
        "id": "l4RKPvV_MgJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Apply the preprocessing function to the comments\n",
        "df['Comment'] = df['Comment'].apply(preprocess_text)\n",
        "\n",
        "# Display the first few rows of the preprocessed dataframe\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z30mC542Mk-V",
        "outputId": "f9da61e0-283e-491e-f473-1c6656090f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Platform                                           Comment  Hateful\n",
            "0   Reddit              damn thought strict gun laws germany        0\n",
            "1   Reddit  dont care stands anything connected like shields        0\n",
            "2   Reddit                                    group idea lol        0\n",
            "3   Reddit                                           america        0\n",
            "4   Reddit  dog spectacular dancer considering two left feet        0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization and Padding:"
      ],
      "metadata": {
        "id": "aK5s555xMqQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features and labels\n",
        "X = df['Comment']\n",
        "y = df['Hateful']\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_tokenized = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Pad the sequences\n",
        "max_length = 100  # Define the maximum length for padding\n",
        "X_padded = pad_sequences(X_tokenized, maxlen=max_length, padding='post')\n",
        "\n",
        "# Display the shape of the padded data\n",
        "print(f'Padded data shape: {X_padded.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tngDNj22MrZs",
        "outputId": "af8387ce-524b-42ce-8956-b920ae9327a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded data shape: (3000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balancing the Dataset with SMOTE:"
      ],
      "metadata": {
        "id": "n9MwhRqbMyYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use SMOTE to balance the dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_padded, y)\n",
        "\n",
        "# Display the shape of the resampled data\n",
        "print(f'Resampled data shape: {X_resampled.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McMCC9qqMz_9",
        "outputId": "75510056-96fe-4762-9248-9ef1cbef5b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resampled data shape: (4800, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the Data:"
      ],
      "metadata": {
        "id": "sGnl9ZjfM7Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the resampled data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'Training samples: {len(X_train)}')\n",
        "print(f'Testing samples: {len(X_test)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYVvZHFfM8yO",
        "outputId": "dcda8cb9-7136-48bf-e4dc-a6c9bcab9895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 3840\n",
            "Testing samples: 960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Model:"
      ],
      "metadata": {
        "id": "4v8004eENBro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "embedding_dim = 128  # Dimension of the embedding vectors\n",
        "model = Sequential()\n",
        "\n",
        "# Add embedding layer\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_length))\n",
        "\n",
        "# Add spatial dropout layer\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "\n",
        "# Add convolutional layer\n",
        "model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Add LSTM layer\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# Add dense layers\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UwuXh9hNE-z",
        "outputId": "7e78f73d-1c0a-4cf5-e315-3e6d3a9e8145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 128)          792704    \n",
            "                                                                 \n",
            " spatial_dropout1d (Spatial  (None, 100, 128)          0         \n",
            " Dropout1D)                                                      \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 96, 64)            41024     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 48, 64)            0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               66000     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 909929 (3.47 MB)\n",
            "Trainable params: 909929 (3.47 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Training the Model:"
      ],
      "metadata": {
        "id": "nxoWpfVkNKaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3Yo6-ZLNLnC",
        "outputId": "b44865b5-9407-4e14-c993-370eeb120f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "48/48 [==============================] - 15s 256ms/step - loss: 0.6939 - accuracy: 0.4827 - val_loss: 0.6931 - val_accuracy: 0.5013\n",
            "Epoch 2/10\n",
            "48/48 [==============================] - 15s 311ms/step - loss: 0.6936 - accuracy: 0.4990 - val_loss: 0.6934 - val_accuracy: 0.4987\n",
            "Epoch 3/10\n",
            "48/48 [==============================] - 10s 208ms/step - loss: 0.6930 - accuracy: 0.5173 - val_loss: 0.6936 - val_accuracy: 0.5013\n",
            "Epoch 4/10\n",
            "48/48 [==============================] - 8s 160ms/step - loss: 0.6937 - accuracy: 0.5003 - val_loss: 0.6932 - val_accuracy: 0.4987\n",
            "Epoch 5/10\n",
            "48/48 [==============================] - 10s 208ms/step - loss: 0.6932 - accuracy: 0.4909 - val_loss: 0.6932 - val_accuracy: 0.5013\n",
            "Epoch 6/10\n",
            "48/48 [==============================] - 9s 184ms/step - loss: 0.6934 - accuracy: 0.4964 - val_loss: 0.6932 - val_accuracy: 0.4987\n",
            "Epoch 7/10\n",
            "48/48 [==============================] - 8s 172ms/step - loss: 0.6932 - accuracy: 0.5033 - val_loss: 0.6932 - val_accuracy: 0.4987\n",
            "Epoch 8/10\n",
            "48/48 [==============================] - 10s 206ms/step - loss: 0.6933 - accuracy: 0.4967 - val_loss: 0.6931 - val_accuracy: 0.5013\n",
            "Epoch 9/10\n",
            "48/48 [==============================] - 8s 166ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6932 - val_accuracy: 0.4987\n",
            "Epoch 10/10\n",
            "48/48 [==============================] - 10s 211ms/step - loss: 0.6933 - accuracy: 0.4902 - val_loss: 0.6932 - val_accuracy: 0.4987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the Model:"
      ],
      "metadata": {
        "id": "GIcAiSrRNQbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0IHUKTpNRtK",
        "outputId": "0a4f6c1d-600f-4083-b1d6-6b578cd3a7d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30/30 [==============================] - 1s 21ms/step\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.6931 - accuracy: 0.4990\n",
            "Test Accuracy: 0.4989583194255829\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       481\n",
            "           1       0.50      1.00      0.67       479\n",
            "\n",
            "    accuracy                           0.50       960\n",
            "   macro avg       0.25      0.50      0.33       960\n",
            "weighted avg       0.25      0.50      0.33       960\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}