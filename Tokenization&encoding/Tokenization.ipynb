{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We will follow these steps:\n",
        "\n",
        "1.Loading the dataset and inspect its contents.\n",
        "\n",
        "2.Performing tokenization using nltk.\n",
        "\n",
        "3.Encoding the text data using TF-IDF."
      ],
      "metadata": {
        "id": "FeCpS4BDafEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: Importing Libraries and Load Dataset"
      ],
      "metadata": {
        "id": "oJvNuM3Za7an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/HateSpeechDetection (preprocessed).csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Display column names to verify\n",
        "print(\"\\nColumn names in the dataset:\")\n",
        "print(df.columns)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"\\nBasic information about the dataset:\")\n",
        "print(df.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7awvTRWbFwc",
        "outputId": "49c4c5ea-1559-4ead-ad3c-db0fc9aae790"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the dataset:\n",
            "  Platform                                            Comment  Hateful\n",
            "0   Reddit  Damn I thought they had strict gun laws in Ger...        0\n",
            "1   Reddit  I dont care about what it stands for or anythi...        0\n",
            "2   Reddit                  It's not a group it's an idea lol        0\n",
            "3   Reddit                          So it's not just America!        0\n",
            "4   Reddit  The dog is a spectacular dancer considering he...        0\n",
            "\n",
            "Column names in the dataset:\n",
            "Index(['Platform', 'Comment', 'Hateful'], dtype='object')\n",
            "\n",
            "Basic information about the dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Platform  3000 non-null   object\n",
            " 1   Comment   3000 non-null   object\n",
            " 2   Hateful   3000 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 70.4+ KB\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Inspect Dataset and Check for Missing Values"
      ],
      "metadata": {
        "id": "7q_U-yZfbsFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information about the dataset\n",
        "print(\"\\nBasic information about the dataset:\")\n",
        "print(df.info())\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values in each column:\\n\", missing_values)\n",
        "\n",
        "# Display the first few rows of the text column\n",
        "# Assuming the text column is named 'processed_text' and the label column is named 'label'\n",
        "text_column = 'Comment'\n",
        "label_column = 'label'\n",
        "\n",
        "print(\"\\nFirst few rows of the text column:\")\n",
        "print(df[text_column].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2vGDUuFbuBb",
        "outputId": "6a5a4673-2bca-4ef7-ea63-22bd1d86772f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Basic information about the dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Platform  3000 non-null   object\n",
            " 1   Comment   3000 non-null   object\n",
            " 2   Hateful   3000 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 70.4+ KB\n",
            "None\n",
            "\n",
            "Missing values in each column:\n",
            " Platform    0\n",
            "Comment     0\n",
            "Hateful     0\n",
            "dtype: int64\n",
            "\n",
            "First few rows of the text column:\n",
            "0    Damn I thought they had strict gun laws in Ger...\n",
            "1    I dont care about what it stands for or anythi...\n",
            "2                    It's not a group it's an idea lol\n",
            "3                            So it's not just America!\n",
            "4    The dog is a spectacular dancer considering he...\n",
            "Name: Comment, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: Tokenization"
      ],
      "metadata": {
        "id": "dZLGlIkGchPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text data\n",
        "df['tokenized_text'] = df[text_column].apply(word_tokenize)\n",
        "\n",
        "# Display the first few rows of the tokenized text column\n",
        "print(\"\\nFirst few rows of the tokenized text column:\")\n",
        "print(df['tokenized_text'].head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spBF1dDIcj81",
        "outputId": "a4266b9a-d24d-4ab7-be29-2e97b48f59f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First few rows of the tokenized text column:\n",
            "0    [Damn, I, thought, they, had, strict, gun, law...\n",
            "1    [I, dont, care, about, what, it, stands, for, ...\n",
            "2       [It, 's, not, a, group, it, 's, an, idea, lol]\n",
            "3                  [So, it, 's, not, just, America, !]\n",
            "4    [The, dog, is, a, spectacular, dancer, conside...\n",
            "5    [If, ppl, dont, wear, masks, you, complain, .....\n",
            "6      [We, should, send, them, All, back, to, africa]\n",
            "7    [Checking, to, see, if, it, 's, whataboutism, ...\n",
            "8            [As, a, european, ,, I, approve, this, .]\n",
            "9    [Idk, which, of, these, groups, to, join, ,, t...\n",
            "Name: tokenized_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: Split Dataset into Training and Testing Sets"
      ],
      "metadata": {
        "id": "-vkk35a4czLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X = df['Comment']\n",
        "y = df['Comment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nTraining set size:\", X_train.shape)\n",
        "print(\"Testing set size:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mAT26Sac0mM",
        "outputId": "e7005c24-e257-448c-f41f-8900ea0828f6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training set size: (2400,)\n",
            "Testing set size: (600,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: TF-IDF Encoding"
      ],
      "metadata": {
        "id": "2VGM6IHgdQHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text data to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(\"\\nTF-IDF feature matrix for training set:\\n\", X_train_tfidf.shape)\n",
        "print(\"TF-IDF feature matrix for testing set:\\n\", X_test_tfidf.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h7yoW0FdRt8",
        "outputId": "04c5eb34-14f7-46d1-8f6c-1a23bbd71432"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF feature matrix for training set:\n",
            " (2400, 5000)\n",
            "TF-IDF feature matrix for testing set:\n",
            " (600, 5000)\n"
          ]
        }
      ]
    }
  ]
}